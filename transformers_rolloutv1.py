# -*- coding: utf-8 -*-
"""Transformers_rolloutV1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vxf8xEM7oOY-gDpMs8U_dLxIh6Mo8lD3

# Classification d'images médicales
## Le projet
Projet de MLOps
Machine learning end to end allant du dataset ( interpretation, pre processing, analyse) en passant par le développement jusqu'au déployement avec une notion d'explicabilité nécessaire dans notre cas d'étude à savoir l'imagerie médicale.
Notre dataset est composé d'images médicales cérébrales à classifier grâce au machine learning. Deux algorithmes basés sur la méthode transformer sont proposés : transformer + heatmap ou transformer + attention rollout. 
## Qu'est ce que transformers? 
Les algorithmes de Transformer avec déploiement d'attention ou cartes thermiques sont des techniques pour visualiser et comprendre comment un modèle de vision Transformer fait des prédictions. Ils peuvent être utiles pour améliorer la transparence et la compréhension des modèles de vision, ce qui peut être important dans des domaines tels que la médecine où les décisions du modèle peuvent avoir des conséquences importantes pour les patients.

Les algorithmes de Transformer avec déploiement d'attention permettent de visualiser les régions de l'image qui ont été les plus influentes pour une prédiction donnée. Cela peut aider à comprendre comment le modèle interagit avec les différentes parties de l'image pour faire des prédictions. Les algorithmes de cartes thermiques fonctionnent de manière similaire, en utilisant une représentation visuelle pour montrer les régions de l'image qui ont été les plus importantes pour une prédiction.

Dans le cas d'un projet de classification de tumeurs cérébrales à l'aide d'un modèle de Vision Transformer, ces algorithmes pourraient être utiles pour comprendre comment le modèle fait des prédictions pour différents types de tumeurs et comment il utilise les informations de l'image pour faire des prédictions. Cela peut aider à identifier des domaines où le modèle peut être amélioré ou des sources potentielles de biais, ce qui peut améliorer la qualité et la fiabilité des résultats.
"""

from google.colab import drive
drive.mount('/content/drive')

"""##Programmation

### Charger les données

####Importation des bibliothèques
"""

# Check the versions of libraries
# Python version
import sys
print('Python: {}'.format(sys.version))
# scipy
import scipy
print('scipy: {}'.format(scipy.__version__))
# numpy
import numpy
print('numpy: {}'.format(numpy.__version__))
# matplotlib
import matplotlib
print('matplotlib: {}'.format(matplotlib.__version__))
# pandas
import pandas
print('pandas: {}'.format(pandas.__version__))
# scikit-learn
import sklearn
print('sklearn: {}'.format(sklearn.__version__))
#tensorflow
import tensorflow
print('tensorflow: {}'.format(tensorflow.__version__))

#Import libraries as
import scipy as sc
import numpy as np
import matplotlib
import pandas as pd
import sklearn as sk
import tensorflow as tf

# Load libraries
#from pandas import read_csv
#from pandas.plotting import scatter_matrix
#from sklearn.model_selection import train_test_split
#from sklearn.model_selection import cross_val_score
#from sklearn.model_selection import StratifiedKFold
#from sklearn.metrics import classification_report
#from sklearn.metrics import confusion_matrix
#from sklearn.metrics import accuracy_score
#from sklearn.linear_model import LogisticRegression
#from sklearn.tree import DecisionTreeClassifier
#from sklearn.neighbors import KNeighborsClassifier
#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
#from sklearn.naive_bayes import GaussianNB
#from sklearn.svm import SVC
from tensorflow import keras
import matplotlib.pyplot as plt

"""####Chargement du dataset"""

#Chemin vers le dataset préalablement chargé dans le drive
path= "/content/drive/MyDrive/Projet_IA_Imagerie/dataset/"
#dataset = keras.utils.image_dataset_from_directory(path, batch_size=64, image_size=(200, 200))
dataset_train = keras.utils.image_dataset_from_directory(path +"Training")
dataset_test = keras.utils.image_dataset_from_directory(path +"Testing")

"""### Résumer l'ensemble des données

1. Dimensions du jeu de données : Repartition des classes et nombre d'images par classes.
"""

print(dataset_test)
class_names=dataset_train.class_names
print(class_names)
nb_train=[826,822,395,827]
nb_test=[100,115,105,74]
total=[926,937,500,901]

#print(dataset_train.take(1))

#np.plot(nb_train)
#for data, labels in dataset_train : 
  #print(data.shape)
  #print(data.dtype)
  #print(labels.shape)
  #print(labels.dtype)
#print(dataset_train.file_paths)

#nb_by_class=

#for data, labels in dataset:
#   print(data.shape)  # (64, 200, 200, 3)
#   print(data.dtype)  # float32
#   print(labels.shape)  # (64,)
#   print(labels.dtype)  # int32

print(type(dataset_train))

"""2. Regardez les données elles-mêmes : Dimension des images

"""

#Visualisation des 9 premieres images

plt.figure(figsize=(10, 10))
for images, labels in dataset_train.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(int(labels[i]))
        plt.axis("off")
        print(np.shape(images[i].numpy()))

"""3. Résumé statistique de tous les attributs : Etude avec une matrice de correlation des pixel ( pixel  corrélé à pixel  ou pas. On va ainsi trouver les zones de l'images dans lesquels il y a des corrélations).
Faire également les tracés statistiques caractérisant nos données.

"""

#caracteristiques de nos données
plt.bar(class_names,nb_train)
plt.title("number of train image per classes")
plt.show()
plt.bar(class_names,nb_test)
plt.title("number of test image per classes")
plt.show()

for i in range(4):
    per_train=(100*nb_train[i])/total[i]
    per_test=(100*nb_test[i])/total[i] 
    plt.pie([per_train,per_test], labels = [f'train {int(per_train)} %', f'test {int(per_test)}%'])
    plt.title(class_names[i])
    plt.legend()
    plt.show()

#Tableau de corrélation
#!pip install dataframe_image
#import dataframe_image as dfi
#dfi.export(df_styled, 'df_styled.png')

"""4. Autres analyses"""

import imageio
pic = images[1].numpy().astype("uint8")
plt.figure(figsize = (5,5))
plt.imshow(pic)
print('Type of the image : ' , type(pic)) 
print('Shape of the image : {}'.format(pic.shape)) 
print('Image Hight {}'.format(pic.shape[0])) 
print('Image Width {}'.format(pic.shape[1])) 
print('Dimension of Image {}'.format(pic.ndim))
print('Image size {}'.format(pic.size)) 
print('Maximum RGB value in this image {}'.format(pic.max())) 

# A specific pixel located at Row : 100 ; Column : 50  
# Each channel's value of it, gradually R , G , B  
print('Value of only R channel {}'.format(pic[ 100, 50, 0])) 
print('Value of only G channel {}'.format(pic[ 100, 50, 1])) 
print('Value of only B channel {}'.format(pic[ 100, 50, 2]))
#images en noir et blanc

np.shape(pic)

"""### Evaluer certains algorithmes

Keras et scikit-learn (Sklearn) sont tous deux des bibliothèques populaires pour le développement de modèles d'apprentissage automatique en Python. Toutefois, ils ont des objectifs différents et des fonctionnalités spécifiques qui les rendent plus appropriés pour certaines tâches.

Keras est une bibliothèque d'apprentissage profond qui se concentre sur la simplification du développement de réseaux de neurones complexes. Il fournit une interface haut niveau pour construire, former et évaluer des modèles d'apprentissage profond en utilisant des couches prédéfinies et des algorithmes optimiseurs. Cela en fait un choix populaire pour les projets de reconnaissance d'images, de traitement du langage naturel et de génération de contenu.

D'un autre côté, Sklearn est une bibliothèque pour les algorithmes d'apprentissage supervisé et non supervisé. Il offre un large éventail d'algorithmes, tels que les régressions, les arbres de décision, les k-means, etc. avec une interface cohérente et simple à utiliser. Sklearn est souvent utilisé pour les projets de classification, de régression et de clustering.

En résumé, Keras peut être un choix plus approprié pour les projets d'apprentissage profond tels que la classification

Pour la suite de l'etude nous ne garderons que 2 classes : no tumor et glioma

#### Transformers rollout
https://keras.io/examples/vision/probing_vits/
"""

#ViT = vission Transformer
!pip install -U gdown -q

import zipfile
from io import BytesIO

import cv2
import gdown
import matplotlib.pyplot as plt
import numpy as np
import requests
import tensorflow as tf
import tensorflow_hub as hub
from PIL import Image
from sklearn.preprocessing import MinMaxScaler
from tensorflow import keras

RESOLUTION = 224
PATCH_SIZE = 16

crop_layer = keras.layers.CenterCrop(RESOLUTION, RESOLUTION)
#norm_layer = keras.layers.Normalization(
#    mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],
#    variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],
#)
rescale_layer = keras.layers.Rescaling(scale=1.0 / 127.5, offset=-1)


def preprocess_image(image, model_type, size=RESOLUTION):
    # Turn the image into a numpy array and add batch dim.
    image = np.array(image)
    image = tf.expand_dims(image, 0)

    # If model type is vit rescale the image to [-1, 1].
    if model_type == "original_vit":
        image = rescale_layer(image)

    # Resize the image using bicubic interpolation.
    resize_size = int((256 / 224) * size)
    image = tf.image.resize(image, (resize_size, resize_size), method="bicubic")

    # Crop the image.
    image = crop_layer(image)

    # If model type is DeiT or DINO normalize the image.
    #if model_type != "original_vit":
    #   image = norm_layer(image)

    return image.numpy()

def get_tfhub_model(model_url: str) -> tf.keras.Model:
    inputs = keras.Input((RESOLUTION, RESOLUTION, 3))
    hub_module = hub.KerasLayer(model_url)
    outputs, attention_weights = hub_module(inputs)
    return keras.Model(inputs, outputs=[outputs, attention_weights])


def get_gdrive_model(model_id: str) -> tf.keras.Model:
    model_path = gdown.download(id=model_id, quiet=False)
    with zipfile.ZipFile(model_path, "r") as zip_ref:
        zip_ref.extractall()
    model_name = model_path.split(".")[0]
    inputs = keras.Input((RESOLUTION, RESOLUTION, 3))
    model = keras.models.load_model(model_name, compile=False)
    outputs, attention_weights = model(inputs)
    return keras.Model(inputs, outputs=[outputs, attention_weights])


def get_model(url_or_id):
    if "https" in url_or_id:
        loaded_model = get_tfhub_model(url_or_id)
    else:
        loaded_model = get_gdrive_model(url_or_id)
    return loaded_model


vit_base_i21k_patch16_224 = get_model("1mbtnliT3jRb3yJUHhbItWw8unfYZw8KJ")
print("Model loaded.")

"""# TEST
Le model VIT etant chargé on doit passer au vision rollout mais avant ça on va retenter de manipuler les données.
https://www.tensorflow.org/tutorials/load_data/images?hl=fr
"""

import numpy as np
import os
import PIL
import PIL.Image
import tensorflow as tf
import tensorflow_datasets as tfds

#On va recharger une image pour qu'elle soit à la bonne taille 224x224
train_ds = keras.utils.image_dataset_from_directory(path+"Training", batch_size=64, image_size=(224, 224))

class_names = train_ds.class_names
print(class_names)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

#une image par classe
"""for i in range(len(class_names)):
    for image, label in train_ds.take(1):
        print(type(image),type(label))
        ax = plt.subplot(3, 3, i+1)
        plt.imshow(image[0].numpy().astype('uint8'))
        plt.title(class_names[label.numpy()[0]])
        plt.axis('off')
"""

""" L' image_batch est un tenseur de la forme (64, 224, 224, 3) . Il s'agit d'un lot de 64 images de forme 224x224x3 (la dernière dimension fait référence aux canaux de couleur RVB). Le label_batch est un tenseur de la forme (64,) , ce sont des labels correspondants aux 64 images.

Vous pouvez appeler .numpy() sur l'un de ces tenseurs pour les convertir en numpy.ndarray .
"""

for image_batch, labels_batch in train_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

image_batch.numpy()

print(np.shape(image_batch.numpy()[1]))
plt.imshow(image_batch.numpy()[1].astype("uint8"))

""" Standardisation des données:

 Les valeurs des canaux RVB sont dans la plage [0, 255] . Ce n'est pas idéal pour un réseau neuronal ; en général, vous devriez chercher à rendre vos valeurs d'entrée petites.

Ici, vous allez normaliser les valeurs pour qu'elles soient dans la plage [0, 1] en utilisant tf.keras.layers.Rescaling :
"""

normalization_layer = tf.keras.layers.Rescaling(1./255)

preprocessed_image=pic
predictions, attention_score_dict = vit_base_i21k_patch16_224.predict(
    preprocessed_image
)
#predicted_label = imagenet_int_to_str[int(np.argmax(predictions))]
#print(predicted_label)

def attention_rollout_map(image, attention_score_dict, model_type):
    num_cls_tokens = 2 if "distilled" in model_type else 1

    # Stack the individual attention matrices from individual Transformer blocks.
    attn_mat = tf.stack([attention_score_dict[k] for k in attention_score_dict.keys()])
    attn_mat = tf.squeeze(attn_mat, axis=1)

    # Average the attention weights across all heads.
    attn_mat = tf.reduce_mean(attn_mat, axis=1)

    # To account for residual connections, we add an identity matrix to the
    # attention matrix and re-normalize the weights.
    residual_attn = tf.eye(attn_mat.shape[1])
    aug_attn_mat = attn_mat + residual_attn
    aug_attn_mat = aug_attn_mat / tf.reduce_sum(aug_attn_mat, axis=-1)[..., None]
    aug_attn_mat = aug_attn_mat.numpy()

    # Recursively multiply the weight matrices.
    joint_attentions = np.zeros(aug_attn_mat.shape)
    joint_attentions[0] = aug_attn_mat[0]

    for n in range(1, aug_attn_mat.shape[0]):
        joint_attentions[n] = np.matmul(aug_attn_mat[n], joint_attentions[n - 1])

    # Attention from the output token to the input space.
    v = joint_attentions[-1]
    grid_size = int(np.sqrt(aug_attn_mat.shape[-1]))
    mask = v[0, num_cls_tokens:].reshape(grid_size, grid_size)
    mask = cv2.resize(mask / mask.max(), image.size)[..., np.newaxis]
    result = (mask * image).astype("uint8")
    return result

attn_rollout_result = attention_rollout_map(
    image, attention_score_dict, model_type="original_vit"
)

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 10))
fig.suptitle(f"Predicted label: {predicted_label}.", fontsize=20)

_ = ax1.imshow(image)
_ = ax2.imshow(attn_rollout_result)
ax1.set_title("Input Image", fontsize=16)
ax2.set_title("Attention Map", fontsize=16)
ax1.axis("off")
ax2.axis("off")

fig.tight_layout()
fig.subplots_adjust(top=1.35)
fig.show()